{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Algorithm for Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association Rule Learning is a prominent area of Data Mining.\n",
    "From a given transcation dataset, the goal is to find a pattern among different items in transactions.\n",
    "This means investigating, to what extent does purchasing a item A leads to purchasing item B. On one hand,\n",
    "it helps the customer finding the items associated with eahc in a store. On the other hand, salesperson can\n",
    "use this information and place associated items together to escalate sales. <br/>\n",
    "One way for Association Rule Learning is Apriori Algorithm.\n",
    "Apriori algorithm uses 2 concepts from Association Rule Mining: <br/>\n",
    "    -Support <br/>\n",
    "    -Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will briefly discuss the two below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a transaction dataset T, Support is the proportion of the transactions that contain both the item A and B. <br/>\n",
    "Support (AB) = P(A $ \\cup $ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a transaction dataset T, Confidence is the percentage of transactions in T, containing A and that also contain B. In other words, this is the probability of B in T given that A is already in that transaction. <br/> Confidence (A $ \\rightarrow $ B) = P(B | A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Apriori algorithm work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A frequent itemset is defined as that subset of items that \"often\" appears together in a single transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori algorithm makes two assumptions:\n",
    "1. All subsets of a frequent itemset must be frequent\n",
    "2. All supersets of an infrequent itemset must also be infrequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires that we should first define a threshold to distinguish between frequent itemset and infrequent itemset. <br\\>\n",
    "Here comes the role of \"Threshold\".\n",
    "We choose to set a value for Threshold. Then, only those items are significant for which the support is more than the set value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the algorithm works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Populate a frequency table of all the unique items that occur in all the transactions.\n",
    "2. Separate the elements for which the support value is less than the threshold. We consider only those items that have the support value greater than the threshold. These items are called 'significant items'.\n",
    "3. Make all possible pairs of the significant items (order does not matter here i.e. AB and BA are one possible pair).\n",
    "4. Compute the fraction of transactions with occurrences of each pair in all the transactions.\n",
    "5. Apply the threshold check again and separate the significant subsets from the insignifacnt ones.\n",
    "6. From the resultant set, we will make itemset of 3 elements in the following fashion:\n",
    "   AB and AC will give ABC\n",
    "   XY and YZ will give XYZ\n",
    "7. We apply the threshold check again and filter the significant itemsets.\n",
    "...\n",
    "This keeps on going till we reach the point from where we cannot make any further frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a repition on threshold check, we can compress the above steps in to two apply a loop on them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Step\\ 1: $ Apply Support threshold check to extarct all the frequent sets with k items in a dataset. <br/>\n",
    "$ Step\\ 2: $ Develop frequent itemset of k+1 items with the help of k-itemsets obtained after the above step. Repeat this process from k=1 to the point when we are unable to obtain any further frequent itemsets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this algorithm is the 'Frequent items set' within a single transaction dataset. We will now see how the association rules can be learned using the Apriori algorithm. Here comes into play the concept of 'Confidence' explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have generated the frequent itemsets, create rules $ only $ from each frequent itemset by  performing binary partition of frequent itemsets and applying filter of high confidence on those rules. These rules are called 'candidate rules'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many and what types of rules are associated to a single frequent itemset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If W is a frequent itemset with k elements then $ 2^{k-2} $ candidate rules are associated with it. <br/>\n",
    "Let's say, ABC is a frequent itemset. Then the associated rules are: <br/>\n",
    "AB $ \\rightarrow $ C <br/> AC $ \\rightarrow $ B <br/> BC $ \\rightarrow $ A <br/> A $ \\rightarrow $ BC <br/> B $ \\rightarrow $ AC <br/> C $ \\rightarrow $ AB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article provides an explanation for the Apriori algorithm for frequent itemset mining and association rule mining. The algorithm is very useful for retail industry in decision making for item placements.\n",
    "This algorithm is very advantageous is a sense that it is very easy to understand, simple to implement and flexible enough to be extended to very large datasets. The bottle neck in its performance is the calculation of Support for each entire dataset needs to be trabersed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
